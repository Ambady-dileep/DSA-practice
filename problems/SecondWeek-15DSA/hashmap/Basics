# table_size = 10
# key = "apple"
# hash_value = hash(key)
# ans = hash_value % table_size
# print(ans)


{
# Hash map v/s Hash set 
# | Feature        | HashMap              | HashSet                    |
# | -------------- | -------------------- | -------------------------- |
# | **Stores**     | Key â†’ Value pairs    | Only values (no keys)      |
# | **Duplicates** | Keys must be unique  | Values must be unique      |
# | **Use Case**   | Lookup by key        | Check existence of a value |
# | **Example**    | `{â€œnameâ€: â€œAmbadyâ€}` | `{â€œappleâ€, â€œbananaâ€}`      |

# Use a HashMap when:
# You need to associate one value with another (e.g., username â†’ password, product â†’ price).

# Use a HashSet when:
# You just care about unique elements and fast lookups (e.g., seen items, blocked users, duplicates remover).
}

{
# What Is Rehashing?

# ğŸ”´ If too many items get crammed into too few buckets â†’ collisions increase, and performance drops.
# âœ… To fix this, rehashing is done:
# Increase the number of buckets (usually double).
# Recalculate (rehash) each key using the new size.
# Re-insert them into the new table.


# ğŸ“Š Example
# Say you have a small hash table of size = 4, and you keep inserting items.

# After a threshold (say 75% full = 3 items), rehashing triggers:

# # Imagine this internal process
# table = [None] * 4  # Original size

# # Adding keys:
# "apple" â†’ index 0  
# "banana" â†’ index 1  
# "mango" â†’ index 1 (collision!)

# Rehashing triggered:
# â¡ Double table size â†’ size = 8
# â¡ Recalculate index for each key using new size
# â¡ Insert each key into new table with updated index

# âš ï¸ Important: Load Factor
# Load Factor = number of items / table size

# If the load factor crosses a threshold (e.g., 0.75), rehashing occurs.

# This ensures O(1) lookup time on average is maintained.

# ğŸ”¥ Real Talk
# In Python, rehashing is automatic in dict and set. You donâ€™t need to do it manually.
# But for interviews, custom hash tables, and performance tuning â†’ you NEED to understand this.
}

{
# Double Hashing (A Type of Collision Resolution)

# Double Hashing is a technique to resolve collisions in open addressing
# Double Hashing - A collision-resolution method using a 2nd hash function
# Python dict/set handles rehashing and resizing internally, but doesnâ€™t use double hashing
# No, double hashing is NOT the automatic resizing (doubling) of the table. Itâ€™s a technique used to handle collisions using a second hash function.
# So, Second Hash Function is used only:
# ğŸ”„ When collision happens.
# During insertion or search (and deletion, if implemented).
# To compute step size and probe for the next available slot.
# If no collision â†’ no second hash needed.
}

{
# Always mention â€œPython uses TimSort which is stableâ€ when asked about sorting â€” it's a pro-level insight that impresses interviewers.
# | Algorithm                    | Stable?           |
# | ---------------------------- | ----------------- |
# | **Bubble Sort**              | âœ… Yes             |
# | **Insertion Sort**           | âœ… Yes             |
# | **Merge Sort**               | âœ… Yes             |
# | **TimSort** (used by Python) | âœ… Yes             |
# | **Quick Sort**               | âŒ No (by default) |
# | **Heap Sort**                | âŒ No              |
# | **Selection Sort**           | âŒ No              |
}

{
# what is Stable sorting?
# What is Timsort?

# | What         | Timsort                                                           |
# | ------------ | ----------------------------------------------------------------- |
# | Combines     | Merge Sort + Insertion Sort                                       |
# | Python Uses  | Yes, for `.sort()` and `sorted()`                                 |
# | Fastest For  | Real-world data, partially sorted, small inputs                   |
# | Stable?      | âœ… Yes (preserves equal-element order)                             |
# | Use It When? | Always â€” it's default in Python, no need to manually implement it |

# âš™ï¸ How Timsort Works â€” Step-by-Step

# ğŸ”¹ Step 1: Divide the data into runs

# A run is a small chunk of data that is already sorted (ascending or descending).
# If not sorted, itâ€™s sorted using insertion sort.
# Usually run size = 32 or 64 elements (platform-dependent)

# ğŸ”¹ Step 2: Sort small runs using Insertion Sort

# Why? Because insertion sort is super fast for small arrays.
# Python sorts these small chunks first.

# ğŸ”¹ Step 3: Merge runs using Merge Sort

# Sorted runs are then merged together.
# Merging is done stably, preserving order of equal items.
# But TimSort doesnâ€™t just blindly merge everything â€” it uses clever rules to merge efficiently (called "galloping mode").

}
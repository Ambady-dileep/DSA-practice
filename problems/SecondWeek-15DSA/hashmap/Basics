# table_size = 10
# key = "apple"
# hash_value = hash(key)
# ans = hash_value % table_size
# print(ans)


{
# Hash map v/s Hash set 
# | Feature        | HashMap              | HashSet                    |
# | -------------- | -------------------- | -------------------------- |
# | **Stores**     | Key → Value pairs    | Only values (no keys)      |
# | **Duplicates** | Keys must be unique  | Values must be unique      |
# | **Use Case**   | Lookup by key        | Check existence of a value |
# | **Example**    | `{“name”: “Ambady”}` | `{“apple”, “banana”}`      |

# Use a HashMap when:
# You need to associate one value with another (e.g., username → password, product → price).

# Use a HashSet when:
# You just care about unique elements and fast lookups (e.g., seen items, blocked users, duplicates remover).
}

{
# What Is Rehashing?

# 🔴 If too many items get crammed into too few buckets → collisions increase, and performance drops.
# ✅ To fix this, rehashing is done:
# Increase the number of buckets (usually double).
# Recalculate (rehash) each key using the new size.
# Re-insert them into the new table.


# 📊 Example
# Say you have a small hash table of size = 4, and you keep inserting items.

# After a threshold (say 75% full = 3 items), rehashing triggers:

# # Imagine this internal process
# table = [None] * 4  # Original size

# # Adding keys:
# "apple" → index 0  
# "banana" → index 1  
# "mango" → index 1 (collision!)

# Rehashing triggered:
# ➡ Double table size → size = 8
# ➡ Recalculate index for each key using new size
# ➡ Insert each key into new table with updated index

# ⚠️ Important: Load Factor
# Load Factor = number of items / table size

# If the load factor crosses a threshold (e.g., 0.75), rehashing occurs.

# This ensures O(1) lookup time on average is maintained.

# 🔥 Real Talk
# In Python, rehashing is automatic in dict and set. You don’t need to do it manually.
# But for interviews, custom hash tables, and performance tuning → you NEED to understand this.
}

{
# Double Hashing (A Type of Collision Resolution)

# Double Hashing is a technique to resolve collisions in open addressing
# Double Hashing - A collision-resolution method using a 2nd hash function
# Python dict/set handles rehashing and resizing internally, but doesn’t use double hashing
# No, double hashing is NOT the automatic resizing (doubling) of the table. It’s a technique used to handle collisions using a second hash function.
# So, Second Hash Function is used only:
# 🔄 When collision happens.
# During insertion or search (and deletion, if implemented).
# To compute step size and probe for the next available slot.
# If no collision → no second hash needed.
}

{
# Always mention “Python uses TimSort which is stable” when asked about sorting — it's a pro-level insight that impresses interviewers.
# | Algorithm                    | Stable?           |
# | ---------------------------- | ----------------- |
# | **Bubble Sort**              | ✅ Yes             |
# | **Insertion Sort**           | ✅ Yes             |
# | **Merge Sort**               | ✅ Yes             |
# | **TimSort** (used by Python) | ✅ Yes             |
# | **Quick Sort**               | ❌ No (by default) |
# | **Heap Sort**                | ❌ No              |
# | **Selection Sort**           | ❌ No              |
}

{
# what is Stable sorting?
# What is Timsort?

# | What         | Timsort                                                           |
# | ------------ | ----------------------------------------------------------------- |
# | Combines     | Merge Sort + Insertion Sort                                       |
# | Python Uses  | Yes, for `.sort()` and `sorted()`                                 |
# | Fastest For  | Real-world data, partially sorted, small inputs                   |
# | Stable?      | ✅ Yes (preserves equal-element order)                             |
# | Use It When? | Always — it's default in Python, no need to manually implement it |

# ⚙️ How Timsort Works — Step-by-Step

# 🔹 Step 1: Divide the data into runs

# A run is a small chunk of data that is already sorted (ascending or descending).
# If not sorted, it’s sorted using insertion sort.
# Usually run size = 32 or 64 elements (platform-dependent)

# 🔹 Step 2: Sort small runs using Insertion Sort

# Why? Because insertion sort is super fast for small arrays.
# Python sorts these small chunks first.

# 🔹 Step 3: Merge runs using Merge Sort

# Sorted runs are then merged together.
# Merging is done stably, preserving order of equal items.
# But TimSort doesn’t just blindly merge everything — it uses clever rules to merge efficiently (called "galloping mode").

}